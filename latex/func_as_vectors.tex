\documentclass[12pt, oneside, letterpaper, fleqn]{article}

\usepackage[title, titletoc]{appendix}
\usepackage{ducky}
\usepackage{tikz}

\renewcommand{\tikz}[1]{\begin{center}\begin{tikzpicture} #1
\end{tikzpicture}\end{center}}
\colorlet{darkblue}{black!36!blue}

\begin{document}
\title{Functions as vectors}
\author{}
\date{\vspace{-5ex}}
\maketitle

\emph{All problems in life can be solved with linear algebra. (almost)}

\vspace{6ex}\textbf{Prereqs}: vectors, matrices, calculus.

Unnecessary\footnote{useless, intuition-inhibiting} rigor is avoided.

\section{Vectors}
\subsection{What are they?}
A picture is worth a thousand words.
\tikz{
\draw[->, blue, very thick] (0,0)--(1.6,1) node[right]{$\vec{a}$};
}

Vectors can be added.
\tikz{
\draw[->, blue, very thick] (0,0)--(1.6,1) node[midway, below]{$\vec{a}$};
\draw[->, red,  very thick] (1.6,1)--(0.8,3) node[midway, right]{$\vec{b}$};
\draw[->, very thick] (0,0)--(0.8,3) node[midway,
left]{$\vec{c}$};
}
$\vec{c} = \vec{a} + \vec{b}$

Vectors exists independently of any coordinate system.

Now pick a coordinate system (pick a \emph{basis}).
\begin{description}
\item[Basis] $\set{\vec{x_1}, \vec{x_2}, \vec{x_3}, \dotsc,
\vec{x_n}}$

\item[Representation] $\vec{a} = a_1\vec{x_1} + a_2\vec{x_2} +
\dotsb + a_n\vec{x_n} = \sum_{i}a_i\vec{x_i} = (a_1, a_2, \dotsc,
a_n)$
\end{description}
\tikz{
\draw[->, ultra thick] (0,0)--(1,0) node[midway, below]{$\vec{x_1}$};
\draw[->, ultra thick] (0,0)--(0.5,2) node[midway, left]{$\vec{x_2}$};
\draw[->, blue, very thick] (3,1)--(6,3) node[above]{$\vec{a}$};
\draw[->, dashed, very thick] (3,1)--(5.5,1) node[midway,
below]{$a_1\vec{x_1}$};
\draw[->, dashed, very thick] (5.5,1)--(6,3) node[midway,
right]{$a_2\vec{x_2}$};
}
Different choices of basis vectors lead to different
\emph{representations}. Once a basis is chosen, the representation of
any vector is unique.

\subsection{Dot product for real vectors}
Some properties:
\begin{itemize}
\item $\vec{a} \cdot \vec{a} > 0$ unless $\vec{a} = 0$ in which case
$\vec{a} \cdot \vec{a} = 0$
\item $\alpha\vec{a} \cdot \vec{b} = \vec{a} \cdot \alpha\vec{b} =
\alpha (\vec{a} \cdot \vec{b})$
\item $\vec{a} \cdot (\vec{b_1} + \vec{b_2}) = \vec{a} \cdot \vec{b_1} +
\vec{a} \cdot \vec{b_2}$
\end{itemize}

\tikz{
\draw[->, blue, very thick] (0,0)--(1.6,1) node[right]{$\vec{a}$};
\draw[->, red, very thick] (0,0)--(1,2) node[above]{$\vec{b}$};
\draw[very thick] (0.53,0.33) arc (32:63.4:0.62);
\draw[] (0.53,0.33) node[above]{$\theta$};
}
Geometric interpretations: length or \emph{norm} of $\vec{a}$ is
$\abs{\vec{a}} = \sqrt{\vec{a} \cdot \vec{a}}$. $\vec{a} \cdot \vec{b} =
\abs{\vec{a}}\abs{\vec{b}} \cos\theta$ (projection of $\vec{a}$ onto
$\vec{b}$ times length of $\vec{b}$, or vice versa).

\subsection{Orthonormal basis}
It is always possible to find a basis where all the basis vectors have
length $1$ and are \emph{orthogonal} (perpendicular) to one another.
A basis $\set{\vec{x_1}, \dotsc, \vec{x_n}}$ where
\begin{align}\label{eq:orthonormal}
\vec{x_i} \cdot \vec{x_j} = \begin{cases}
1 & \text{if } i = j\\
0 & \text{if } i \neq j
\end{cases}
\end{align}
is called an \emph{orthonormal basis}.

Dot products: let $\vec{a} = a_1\vec{x_1} +\dotsb+ a_n\vec{x_n}$ and
$\vec{b} = b_1\vec{x_1} +\dotsb+ b_n\vec{x_n}$ be vectors and their
representations in an orthonormal basis.
\begin{align}
\vec{a}\cdot\vec{b} = a_1b_1 + a_2b_2 + \dotsb + a_nb_n
\end{align}
using \eqref{eq:orthonormal}.

\emph{\textbf{We will only use orthonormal
basis from now on, since it is always possible to convert a basis into
an orthonormal one.}}

\subsection{Matrix multiplication}
\begin{align*}
C = \begin{pmatrix}
c_{11} & \hdots & c_{1j} & \hdots & c_{1n}\\
\vdots & \ddots & \vdots & \ddots & \vdots\\
c_{i1} & \hdots & c_{ij} & \hdots & c_{in}\\
\vdots & \ddots & \vdots & \ddots & \vdots\\
c_{m1} & \hdots & c_{mj} & \hdots & c_{mn}
\end{pmatrix}
\end{align*}
Suppose we have matrices $A$ and $B$ and we want to compute their
product $C = AB$. Matrix multiplication is defined as
\begin{align}
c_{ij} = \sum_k a_{ik}b_{kj}
\end{align}

If we used an orthonormal basis and we wanted to compute the dot product
$\vec{a} \cdot \vec{b}$, we could put their representations into
matrices and compute
\begin{align}
\vec{a} \cdot \vec{b} = \begin{pmatrix}
a_1 \hdots a_n
\end{pmatrix}
\begin{pmatrix}
b_1\\
\vdots\\
b_n
\end{pmatrix}
= a_1b_1 + \dotsb + a_nb_n
\end{align}

\subsection{Dirac's notation}
Dirac's notation for vectors distinguishes between row vectors and column
vectors. Row vector:
\begin{align}
\bra{a} = \begin{pmatrix}a_1 \hdots a_n\end{pmatrix}
\end{align}
Column vector:
\begin{align}
\ket{b} = \begin{pmatrix}b_1\\
\vdots\\
b_n
\end{pmatrix}
\end{align}

$\vec{a} \cdot \vec{b}$ is now written as $\braket{a}{b}$.

In old notation, $\vec{a} = a_1\vec{x_1} +\dotsb+ a_n\vec{x_n}$, and
$a_i = \vec{x_i} \cdot \vec{a}$. In Dirac's notation, $\ket{a} =
a_1\ket{x_1} +\dotsb+ a_n\ket{x_n}$, and $a_i = \braket{x_i}{a}$. We see
\begin{align*}
\ket{a} &= a_1\ket{x_1} +\dotsb+ a_n\ket{x_n}\\
&= \ket{x_1}\braket{x_1}{a} +\dotsb+ \ket{x_n}\braket{x_n}{a}\\
&= \left(\ketbra{x_1}{x_1} +\dotsb+ \ketbra{x_n}{x_n}\right)\ket{a}
\end{align*}
from which we conclude
\begin{align}\begin{split}\label{eq:finite_id}
1 &= \ketbra{x_1}{x_1} + \ketbra{x_2}{x_2} +\dotsb+ \ketbra{x_n}{x_n}\\
&= \sum_k \ketbra{x_k}{x_k}
\end{split}\end{align}

\subsection{Complex vectors}
When we allow vectors to be multiplied by complex numbers, we still want
to keep the idea of the length of a vector. For real vectors, this was
$\sqrt{\braket{a}{a}}$, and we would like the same definition for
complex vectors. But $a_1^2 + a_2^2 + \dotsb + a_n^2$ is not guaranteed
to be real, and we would like length to be a nonnegative real number.
However, the quantity $\abs{a_1}^2 = \bar{a_1}a_1$ is always real and
nonnegative, where $\bar{a_1}$ refers to the complex conjugate of $a_1$. So
we extend our definition of dot product to complex vectors by doing
this:
\begin{align}
\braket{a}{b} = \bar{a_1}b_1 + \dotsb + \bar{a_n}b_n
\end{align}
If the $a_i$'s were real numbers, $\bar{a_i} = a_i$ and we recover our
old formula. But now $\braket{a}{a}$ is guaranteed to be a nonnegative
real number for all complex vectors.

So now we have the following rule:
\begin{align}
\begin{split}
\text{If }\ket{a} = \begin{pmatrix}a_1\\
\vdots\\
a_n\end{pmatrix}
\text{, then }\bra{a} = \begin{pmatrix}\bar{a_1} \hdots
\bar{a_n}\end{pmatrix}
\end{split}
\end{align}
and vice versa. Define $\bar{\ket{a}} = \bra{a}$ and $\bar{\bra{a}} =
\ket{a}$. We see that
\begin{align}
\begin{split}
\bar{\braket{a}{b}} &= \bar{\bar{a_1}b_1 + \dotsb +
\bar{a_n}b_n}\\
&= a_1\bar{b_1} + \dotsb + a_n\bar{b_n}\\
&= \braket{b}{a}
\end{split}
\end{align}
so complex conjugation of a product is complex conjugation of individual
parts of the product but everything written backwards.
\footnote{Usually this is referred to as \emph{adjoint} or
\emph{conjugate transpose} and written with $\dagger$ rather than a
bar.}

\section{Functions}
\subsection{Vectors as functions}
A function looks like $f$. A value, $f(x)$ is assigned to all $x$ in
the domain. $f(1.2) = \text{blah}$, $f(\pi) = \text{blahblah}$, etc.

Compare with vector representations: $a_1 = \text{blah}$, $a_2 =
\text{blahblah}$. A value is assigned to every index. We can construct a
function $a$ where $a(1) = a_1$, $a(2) = a_2$, etc. So the function $a$
is the representation of the vector $\ket{a}$ in some basis.

\tikz{
\draw[->, very thick] (0,0)--(7,0) node[right]{$i$};
\draw[->, very thick] (0,0)--(0,3) node[above]{$a(i)$};
\draw[darkblue] (2,0)--(2,1) node[above]{$a(1)$};
\filldraw[darkblue] (2,1) circle (0.05);
\draw[darkblue] (4,0)--(4,2.5) node[above]{$a(2)$};
\filldraw[darkblue] (4,2.5) circle (0.05);
\draw[darkblue] (6,0)--(6,2) node[above]{$a(3)$};
\filldraw[darkblue] (6,2) circle (0.05);
}

\subsection{Functions as vectors}
\tikz{
\draw[->, very thick] (0,0)--(7,0) node[right]{$x$};
\draw[->, very thick] (0,0)--(0,3) node[above]{$f(x)$};
\draw[darkblue, thick] (0,1.8) .. controls (1.5,3) and (4,0) .. (6,1.5);
}

Now the reverse. From the function $f$ we can construct a vector
$\ket{f}$, so $f$ is the representation of $\ket{f}$. The analogy:
\begin{align*}
\ket{a}& & \ket{f}&\\
\braket{x_2}{a} &= a_2 & \braket{x_2}{f} &= f(2)\\
\braket{a}{b} &= \sum_k \bar{a_k}b_k & \braket{f}{g} &= ???
\end{align*}
The problem is that the domain of $f$ is continuous, so we can't just
form a sum for $\braket{f}{g}$ as we did for $\braket{a}{b}$ because it
would be an infinite sum that in general does not converge. Instead we
shall do this
\begin{align}\label{eq:innerp}
\braket{f}{g} = \int \bar{f(x)}g(x)\,dx
\end{align}
This preserves the spirit of the dot product, the only change being the
weight $dx$ which converts infinite quantities to finite.

Some notation notes: $\braket{x_2}{f} = f(2)$, $\braket{x_\pi}{f} =
f(\pi)$, etc., but to avoid the awkward notation of $\braket{x_x}{f} =
f(x)$, we shall just write $\braket{x}{f} = f(x)$.

Continuing with \eqref{eq:innerp}
\begin{align*}
\braket{f}{g} &= \int \bar{f(x)}g(x)\,dx\\
&= \int \braket{f}{x}\braket{x}{g}\,dx\\
&= \bra{f} \left(\int \ketbra{x}{x}\,dx\right) \ket{g}
\end{align*}
we see that
\begin{align}\label{eq:inf_id}
1 = \int \ketbra{x}{x}\,dx
\end{align}
analogous to \eqref{eq:finite_id}.

\subsection{Dirac's delta function}
What is $\braket{x}{x_\pi}$ or $\braket{x}{x_1}$? It's going to be a
function, just like $\braket{x}{f} = f(x)$ is a function. So let's just
write $\delta_\pi(x) = \braket{x}{x_\pi}$, $\delta_2(x) =
\braket{x}{x_2}$, etc.

Using \eqref{eq:inf_id}
\begin{align*}
\bar{f(\pi)} = \braket{f}{x_\pi} &= \int
\braket{f}{x}\braket{x}{x_\pi}\,dx\\
&= \int \bar{f(x)} \delta_\pi(x)\,dx
\end{align*}
Since this works for any $f$, we must have
\footnote{Unnecessary rigor is avoided. It is possible to avoid
defining the $\delta$ function completely and continue on without it,
but the analogy with vectors in finite dimensional spaces is lost.
Intuition is more important than rigor, and the $\delta$ function is
a very useful idea.}
\begin{align*}
\braket{x}{x_\pi} = \delta_\pi(x) = \begin{cases}
\frac{1}{dx} & \text{for } \pi-\frac{dx}{2} \leq x \leq
\pi+\frac{dx}{2}\\
0 & \text{everywhere else}
\end{cases}
\end{align*}

How small/What is $dx$? $dx$ shall be a quantity that is smaller than the
precision of your data collecting tools (ruler, weight scale,
voltmeter, experimental apparati) or if you're thinking about it
theoretically, smaller than anything you can imagine.

Notation notes: Instead of writing $\delta_\pi(x)$ or $\delta_2(x)$, we
can just write $\delta_0(x-\pi)$ and $\delta_0(x-2)$. Then we can just
drop the subscript $0$ and just write $\delta(x)$ to mean $\delta_0(x)$
and $\delta(x-\pi) = \delta_\pi(x)$, etc.

\tikz{
\draw[->, very thick] (-1,0)--(5,0) node[right]{$x$};
\draw[->, very thick] (0,-1)--(0,4) node[above]{$\delta(x-\pi)$};
\draw[black] (3.14,0) node[below, black]{$\pi$} rectangle (3.24,3.5)
node[midway, right]{$\frac{1}{dx}$} node[above]{$dx$};
}

\subsection{The differential operator}
So we've established the notation $f(x) = \braket{x}{f}$. How are we
going to write $\odx{f} = f'(x)$? $f'(\pi) = (f(\pi+dx) - f(\pi))/dx$,
which is $(\braket{x_{\pi+dx}}{f} - \braket{x_\pi}{f})/dx$. Take a look
at \eqref{eq:finite_deriv}, where we have let $D$ be the giant matrix

\begin{align}\begin{split}\label{eq:finite_deriv}
D \ket{a} &=
\begin{pmatrix}
-1 & 1 & 0 & 0 & 0\\
0 & -1 & 1 & 0 & 0\\
0 & 0 & -1 & 1 & 0\\
0 & 0 & 0 & -1 & 1\\
0 & 0 & 0 & 0 & 1\\
\end{pmatrix}
\begin{pmatrix}
a_1\\
a_2\\
a_3\\
a_4\\
a_5
\end{pmatrix}\\
&=
\begin{pmatrix}
a_2 - a_1\\
a_3 - a_2\\
a_4 - a_3\\
a_5 - a_4\\
a_5
\end{pmatrix}
\end{split}\end{align}
Apart from the last element of $D\ket{a}$, all other elements are of
the form $\bra{x_k}D\ket{a} = \braket{x_{k+1}}{a} - \braket{x_k}{a}$.
Comparing with $f'(\pi) = (\braket{x_{\pi+dx}}{f} -
\braket{x_\pi}{f})/dx$, we can imagine $\odx{}$ to be a matrix
that looks kinda like $D$ in \eqref{eq:finite_deriv}.
\begin{align}
f'(x) = \bra{x}\odx{}\ket{f}
\end{align}

What about derivative of the delta function, like
$\bra{x}\odx{}\ket{x_\pi} = \delta'(x - \pi)$? It's actually more useful
to look at dot products involving the delta function's derivative.
\begin{align*}
\bra{f}\odx{}\ket{x_\pi} &= \int
\braket{f}{x}\bra{x}\odx{}\ket{x_\pi}\,dx\\
&= \int \bar{f(x)} \delta'(x-\pi)\,dx\\
\intertext{This can be integrated by parts.}
&= \left[\bar{f(x)} \delta(x-\pi)\right] - \int \bar{f'(x)}
\delta(x-\pi)\,dx\\
\intertext{The delta function is $0$ at the limits of integration so}
&= -\int \bar{f'(x)}\delta(x-\pi)\,dx\\
&= -\bar{f'(\pi)}\\
&= -\bar{\bra{x_\pi}\odx{}\ket{f}}\\
&= -\bra{f}\bar{\odx{}}\ket{x_\pi}
\end{align*}

So we have established the following $2$ relations
\begin{align}
\bra{f}\odx{}\ket{x_\pi} &= -\bar{f'(\pi)}\\
\intertext{and}
\bar{\odx{}} &= -\odx{}
\end{align}

\section{Change of basis}
\subsection{Rotation}


\section{Variational calculus as multivariable calculus}
\subsection{Application to physics}
\subsubsection{The Lagrangian and Newton's second law}
Let $L(x(t), \dot{x}(t), t) = \frac{1}{2}m\dot{x}^2 - U(x)$. The
Euler-Lagrange equation yields $F = m\ddot{x} = ma$, a.k.a.\ Newton's
second law. The quantity $(\text{kinetic energy} - \text{potential
energy})$ is the \emph{Lagrangian}.

\subsubsection{The Hamiltonian and Hamilton's equations of motion}
Computing the derivatives with chain rule and product rule
\begin{align*}
\odt{L} &= \dot{x}\pdx{L} + \ddot{x}\pd{L}{\dot{x}} + \pdt{L}\\
\odt{}\left(\dot{x}\pd{L}{\dot{x}}\right) &= \ddot{x}\pd{L}{\dot{x}} +
\dot{x}\odt{}\pd{L}{\dot{x}}
\end{align*}

We can form
\begin{align*}
\odt{L} - \odt{}\left(\dot{x}\pd{L}{\dot{x}}\right) =
\dot{x}\left(\pdx{L} - \odt{}\pd{L}{\dot{x}}\right) + \pdt{L}
\end{align*}
which by Euler-Lagrange becomes
\begin{align*}
\odt{}\left(L - \dot{x}\pd{L}{\dot{x}}\right) = \pdt{L}
\end{align*}
The quantity $\dot{x}\pd{L}{\dot{x}} - L$ is the
\emph{Hamiltonian} (denoted $H$), which we see is a constant of motion
as long as $\pdt{L} = 0$, i.e.\ $L$ does not involve time explicitly.

Define $p = \pd{L}{\dot{x}}$, the \emph{canonical momentum}. The
Hamiltonian is $H = \dot{x}p - L$. Looking at its differential
\begin{align*}
dH &= p\,d\dot{x} + \dot{x}\,dp - dL\\
&= p\,d\dot{x} + \dot{x}\,dp - \left(\pdx{L}\,dx + p\,d\dot{x} +
\pdt{L}\,dt\right)\\
&= \dot{x}\,dp - \pdx{L}\,dx - \pdt{L}\,dt\\
&= \dot{x}\,dp - \dot{p}\,dx - \pdt{L}\,dt
\end{align*}
where in the last step we have used Euler-Lagrange ($\dot{p} =
\odt{}\pd{L}{\dot{x}} = \pdx{L}$). We can read off some of $H$'s partial
derivatives
\begin{align}\begin{split}\label{eq:hamilton}
\dot{x} &= \pd{H}{p}\\
\dot{p} &= -\pd{H}{x}
\end{split}\end{align}
which are \emph{Hamilton's equations of motion}.

\subsubsection{The Poisson bracket}
Suppose we have a dynamical variable $b(x,p)$, which could be kinetic
energy, position, momentum, angular momentum (if you extend this to 3
dimensions), whatever. Knowing the initial value of $b$ and $\odt{b}$
for all time would solve physics.
\begin{align*}
\odt{b} &= \pdx{b}\dot{x} + \pd{b}{p}\dot{p}\\
&= \pdx{b}\pd{H}{p} - \pd{b}{p}\pdx{H}
\end{align*}
using \eqref{eq:hamilton}.

Define the \emph{Poisson bracket}
\begin{align}
[m, n] = \pdx{m}\pd{n}{p} - \pdx{n}\pd{m}{p}
\end{align}
Then we have
\begin{align}\label{eq:classical_eom}
\odt{b} = [b, H]
\end{align}
If you know the Hamiltonian of the system and you know the initial value
of any dynamical variable $b$ (that is explicitly independent of time),
you know $b$ for all time (also see \eqref{eq:quantum_eom}). In other
words, you win.

\subsubsection{Quantum Poisson bracket and Schr\"odinger's
equation}\label{sec:quantum}
The passage of classical mechanics to quantum mechanics would require
its own textbook. This section will therefore not make any sense except
to people already familiar with basic quantum mechanics.

For simplicity, let's consider an isolated system (constant $H$). A
nonconstant $H$ would just mean we didn't take some interaction in our
system into account properly.

Define the \emph{quantum Poisson bracket} of two dynamical variables $m$
and $n$\footnote{Usually the notation $[m,n]$ refers to the commutator
$mn-nm$. Usually, but not here.}

\begin{align}
[m, n] = \frac{mn - nm}{i\hbar}
\end{align}
where $\hbar$ is a constant of nature. As an experimentally verified
fact of life (physical law), if we have a dynamical variable $b$
explicitly independent of time (in the sense that its evolution is only
a result of internal interactions)
\begin{align}\label{eq:quantum_eom}
\odt{b} = [b, H]
\end{align}
which is identical in form to \eqref{eq:classical_eom} except we use
the quantum Poisson bracket here.

Denote $b_0$ to be the initial value of $b$ and $b_t$ to be the value
after time $t$.\footnote{$b_t$ is written instead of $b(t)$ to emphasize
that $b$ changes only as a result of physical interactions} It can be
shown that $b_t$ is related to $b_0$ by a unitary change of basis $T(t)$
independent of $b$ (see
\hyperref[app:complicated]{Appendix~\ref*{app:complicated}}).
\begin{align}
b_t = T\inv b_0 T = \bar{T} b_0 T
\end{align}
Note that $H_t = H_0$ ($T$ commutes with $H$) as we can use
\eqref{eq:quantum_eom} and $[H, H] = 0$).

Plugging back into \eqref{eq:quantum_eom}

\begin{align*}
\odt{\bar{T}} b_0 T + \bar{T} b_0 \odt{T}
= \frac{\bar{T}b_0TH}{i\hbar} - \frac{H\bar{T}b_0T}{i\hbar}\\
\end{align*}
which can be written as
\begin{align}\label{eq:qm_eom_explicit}
\bar{\bar{T} b_0 \odt{T}}+ \bar{T} b_0 \odt{T}
= \bar{\frac{\bar{T}b_0HT}{i\hbar}} +
\frac{\bar{T}b_0HT}{i\hbar} 
\end{align}

From \eqref{eq:qm_eom_explicit} we can identify $i\hbar \odt{T} =
HT$ (see \hyperref[app:complicated]{Appendix~\ref*{app:complicated}}
for a less shaky approach; a counterexample to the argument $a + \bar{a}
= b + \bar{b} \implies a = b$ is $0 + \bar{0} = i + \bar{i}$). If we write
\begin{align}\label{eq:schrodinger_psi}
T\ket{\psi} = \ket{\psi_t}
\end{align}
then
\begin{align}\label{eq:schrodinger_equation}
i\hbar \odt{T} \ket{\psi} = HT\ket{\psi}\nonumber\\
i\hbar \odt{}\ket{\psi_t} = H\ket{\psi_t}
\end{align}
which is the \emph{Schr\"odinger equation}.

\hfill \emph{Last modified: Bryance Oyang, \today}

\begin{appendices}
\section{Unhandwaiving the handwaiving}\label{app:complicated}
Proof by appendix.

\section{But seriously}
We're trying to get the Schr\"odinger equation (assuming a constant
$H$), expanding on an analogy with the classical equation of motion
\eqref{eq:classical_eom}.

This is best done using diagrams (as is anything involving tensor
products and products of tensors) but currently I don't feel like
drawing them. So here it is using math symbols.

Start with \eqref{eq:quantum_eom}.
\begin{align*}
\odt{b} = \frac{1}{i\hbar}(bH - Hb)
\end{align*}
We can reshape $b$ (a matrix) into a vector (sorta like MATLAB's
\texttt{reshape} command). Then we'll have something like
\begin{align*}
\odt{b} = \frac{1}{i\hbar}(1\otimes H^\intercal - H\otimes 1)b
\end{align*}
where $H^\intercal$ is the transpose of $H$ and $\otimes$ is tensor product.

We can recognize this as a differential equation of the form
$\odt{\vec{v}} = M\vec{v}$ with solution $\vec{v} = \exp(Mt)\vec{v_0}$.
\begin{align*}
b_t &= \exp\left(\frac{t}{i\hbar}(1\otimes H^\intercal - H\otimes 1)\right)b_0\\
&= \exp\left(\frac{t}{i\hbar}1\otimes H^\intercal\right)
\exp\left(\frac{-t}{i\hbar}H\otimes 1\right)b_0
\end{align*}
as $1\otimes H^\intercal$ and $H\otimes 1$ commute. This can be written as
\begin{align*}
b_t &= \exp\left(\frac{-t}{i\hbar}H\right)
\otimes\exp\left(\frac{t}{i\hbar}H^\intercal\right)b_0
\end{align*}

Reshaping this back into the original form where $b$ was a matrix
\begin{align*}
b_t &= \exp\left(\frac{-t}{i\hbar}H\right) b_0
\exp\left(\frac{t}{i\hbar}H\right)\\
&= \bar{T} b_0 T
\end{align*}
letting $T = \exp\left(\frac{t}{i\hbar}H\right)$. We recognize $T$ as
the time evolution operator. We can compute its derivative
\begin{align*}
\odt{T} = \frac{1}{i\hbar} HT
\end{align*}
after which \eqref{eq:schrodinger_psi}--\eqref{eq:schrodinger_equation}
should be clear.
\end{appendices}

\end{document}
