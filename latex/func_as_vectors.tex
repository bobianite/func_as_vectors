\documentclass[12pt, oneside, letterpaper, fleqn]{article}

\usepackage{bobianite}

\colorlet{darkblue}{black!36!blue}
\colorlet{darkgreen}{black!36!green}

\begin{document}
\title{Functions as vectors}
\author{}
\date{\vspace{-5ex}}
\maketitle

\emph{All problems in life can be solved with linear algebra. (almost)}

\vspace{6ex}\textbf{Prereqs}: vectors, matrices, calculus.

\section{Vectors}
\subsection{What are they?}
A picture is worth a thousand words.
\tikz{
\draw[->, blue, very thick] (0,0)--(1.6,1) node[right]{$\vec{a}$};
}

Vectors can be added.
\tikz{
\draw[->, blue, very thick] (0,0)--(1.6,1) node[midway, below]{$\vec{a}$};
\draw[->, red,  very thick] (1.6,1)--(0.8,3) node[midway, right]{$\vec{b}$};
\draw[->, very thick] (0,0)--(0.8,3) node[midway,
left]{$\vec{c}$};
}
$\vec{c} = \vec{a} + \vec{b}$

Vectors exists independently of any coordinate system.

Now pick a coordinate system (pick a \emph{basis}).
\begin{description}
\item[Basis] $\set{\vec{x_1}, \vec{x_2}, \vec{x_3}, \dotsc,
\vec{x_n}}$

\item[Representation] $\vec{a} = a_1\vec{x_1} + a_2\vec{x_2} +
\dotsb + a_n\vec{x_n} = \sum_k a_k\vec{x_k} = (a_1, a_2, \dotsc,
a_n)$
\end{description}
\tikz{
\draw[->, ultra thick] (0,0)--(1,0) node[midway, below]{$\vec{x_1}$};
\draw[->, ultra thick] (0,0)--(0.5,2) node[midway, left]{$\vec{x_2}$};
\draw[->, blue, very thick] (3,1)--(6,3) node[above]{$\vec{a}$};
\draw[->, dashed, very thick] (3,1)--(5.5,1) node[midway,
below]{$a_1\vec{x_1}$};
\draw[->, dashed, very thick] (5.5,1)--(6,3) node[midway,
right]{$a_2\vec{x_2}$};
}
Different choices of basis vectors lead to different
\emph{representations}. Once a basis is chosen, the representation of
any vector is unique.

\subsection{Dot product for real vectors}
Some properties:
\begin{itemize}
\item $\vec{a} \cdot \vec{a} > 0$ unless $\vec{a} = 0$ in which case
$\vec{a} \cdot \vec{a} = 0$
\item $\alpha\vec{a} \cdot \vec{b} = \vec{a} \cdot \alpha\vec{b} =
\alpha (\vec{a} \cdot \vec{b})$
\item $\vec{a} \cdot (\vec{b_1} + \vec{b_2}) = \vec{a} \cdot \vec{b_1} +
\vec{a} \cdot \vec{b_2}$
\end{itemize}

\tikz{
\draw[->, blue, very thick] (0,0)--(1.6,1) node[right]{$\vec{a}$};
\draw[->, red, very thick] (0,0)--(1,2) node[above]{$\vec{b}$};
\draw[very thick] (0.53,0.33) arc (32:63.4:0.62);
\draw (0.53,0.33) node[above]{$\theta$};
}
Geometric interpretations: length or \emph{norm} of $\vec{a}$ is
$\abs{\vec{a}} = \sqrt{\vec{a} \cdot \vec{a}}$. $\vec{a} \cdot \vec{b} =
\abs{\vec{a}}\abs{\vec{b}} \cos\theta$ (projection of $\vec{a}$ onto
$\vec{b}$ times length of $\vec{b}$, or vice versa).

\subsection{Orthonormal basis}
It is always possible to find a basis where all the basis vectors have
length $1$ and are \emph{orthogonal} (perpendicular) to one another.
A basis $\set{\vec{x_1}, \dotsc, \vec{x_n}}$ where
\begin{align}\label{eq:orthonormal}
\vec{x_i} \cdot \vec{x_j} = \begin{cases}
1 & \text{if } i = j\\
0 & \text{if } i \neq j
\end{cases}
\end{align}
is an \emph{orthonormal basis}.

Dot products: let $\vec{a} = a_1\vec{x_1} +\dotsb+ a_n\vec{x_n}$ and
$\vec{b} = b_1\vec{x_1} +\dotsb+ b_n\vec{x_n}$ be vectors and their
representations in an orthonormal basis.
\begin{align}
\vec{a}\cdot\vec{b} = a_1b_1 + a_2b_2 + \dotsb + a_nb_n
\end{align}
using \eqref{eq:orthonormal}.

\emph{\textbf{We will only use orthonormal
basis from now on, since it is always possible to convert a basis into
an orthonormal one.}}

\subsection{Matrix multiplication}
\begin{align*}
C = \begin{pmatrix}
c_{11} & \hdots & c_{1j} & \hdots & c_{1n}\\
\vdots & \ddots & \vdots & \ddots & \vdots\\
c_{i1} & \hdots & c_{ij} & \hdots & c_{in}\\
\vdots & \ddots & \vdots & \ddots & \vdots\\
c_{m1} & \hdots & c_{mj} & \hdots & c_{mn}
\end{pmatrix}
\end{align*}
Suppose we have matrices $A$ and $B$ and we want to compute their
product $C = AB$. Matrix multiplication is defined as
\begin{align}
c_{ij} = \sum_k a_{ik}b_{kj}
\end{align}

If we used an orthonormal basis and we wanted to compute the dot product
$\vec{a} \cdot \vec{b}$, we could put their representations into
matrices and compute
\begin{align}
\vec{a} \cdot \vec{b} = \begin{pmatrix}
a_1 \hdots a_n
\end{pmatrix}
\begin{pmatrix}
b_1\\
\vdots\\
b_n
\end{pmatrix}
= a_1b_1 + \dotsb + a_nb_n
\end{align}

\subsection{Dirac's notation}
Dirac's notation for vectors distinguishes between row vectors and column
vectors. Row vector:
\begin{align}
\bra{a} = \begin{pmatrix}a_1 \hdots a_n\end{pmatrix}
\end{align}
Column vector:
\begin{align}
\ket{b} = \begin{pmatrix}b_1\\
\vdots\\
b_n
\end{pmatrix}
\end{align}

$\vec{a} \cdot \vec{b}$ is now written as $\braket{a}{b}$.

In old notation, $\vec{a} = a_1\vec{x_1} +\dotsb+ a_n\vec{x_n}$, and
$a_k = \vec{x_k} \cdot \vec{a}$. In Dirac's notation, $\ket{a} =
a_1\ket{x_1} +\dotsb+ a_n\ket{x_n}$, and $a_k = \braket{x_k}{a}$. We see
\begin{align*}
\ket{a} &= a_1\ket{x_1} +\dotsb+ a_n\ket{x_n}\\
&= \ket{x_1}\braket{x_1}{a} +\dotsb+ \ket{x_n}\braket{x_n}{a}\\
&= \left(\ketbra{x_1}{x_1} +\dotsb+ \ketbra{x_n}{x_n}\right)\ket{a}
\end{align*}
from which we conclude
\begin{align}\begin{split}\label{eq:finite_id}
1 &= \ketbra{x_1}{x_1} + \ketbra{x_2}{x_2} +\dotsb+ \ketbra{x_n}{x_n}\\
&= \sum_k \ketbra{x_k}{x_k}
\end{split}\end{align}

\subsection{Complex vectors}
When we allow vectors to be multiplied by complex numbers, we still want
to keep the idea of the length of a vector. For real vectors, this was
$\sqrt{\braket{a}{a}}$, and we would like the same definition for
complex vectors. But $a_1^2 + a_2^2 + \dotsb + a_n^2$ is not guaranteed
to be real, and we would like length to be a nonnegative real number.
However, the quantity $\abs{a_1}^2 = \bar{a_1}a_1$ is always real and
nonnegative, where $\bar{a_1}$ refers to the complex conjugate of $a_1$. So
we extend our definition of dot product to complex vectors by doing
this:
\begin{align}
\braket{a}{b} = \bar{a_1}b_1 + \dotsb + \bar{a_n}b_n
\end{align}
If the $a_k$'s were real numbers, $\bar{a_k} = a_k$ and we recover our
old formula. But now $\braket{a}{a}$ is guaranteed to be a nonnegative
real number for all complex vectors.

So now we have the following rule:
\begin{align}
\begin{split}
\text{If }\ket{a} = \begin{pmatrix}a_1\\
\vdots\\
a_n\end{pmatrix}
\text{, then }\bra{a} = \begin{pmatrix}\bar{a_1} \hdots
\bar{a_n}\end{pmatrix}
\end{split}
\end{align}
and vice versa. Define $\bar{\ket{a}} = \bra{a}$ and $\bar{\bra{a}} =
\ket{a}$. We see that
\begin{align}
\begin{split}
\bar{\braket{a}{b}} &= \bar{\bar{a_1}b_1 + \dotsb +
\bar{a_n}b_n}\\
&= a_1\bar{b_1} + \dotsb + a_n\bar{b_n}\\
&= \braket{b}{a}
\end{split}
\end{align}
so \emph{complex conjugation of a product is complex conjugation of
individual parts of the product but everything written
backwards}.\footnote{Usually this is referred to as \emph{adjoint} or
\emph{conjugate transpose} and written with $\dagger$ rather than a
bar.}

\section{Functions}
\subsection{Vectors as functions}
A function looks like $f$. A value, $f(x)$ is assigned to all $x$ in
the domain. $f(1.2) = \text{blah}$, $f(\pi) = \text{blahblah}$, etc.

Compare with vector representations: $a_1 = \text{blah}$, $a_2 =
\text{blahblah}$. A value is assigned to every index. We can construct a
function $a$ where $a(1) = a_1$, $a(2) = a_2$, etc. So the function $a$
is the representation of the vector $\ket{a}$ in some basis.

\tikz{
\draw[->, very thick] (0,0)--(7,0) node[right]{$k$};
\draw[->, very thick] (0,0)--(0,3) node[above]{$a(k)$};
\draw[darkblue] (2,0)--(2,1) node[above]{$a(1)$};
\filldraw[darkblue] (2,1) circle (0.05);
\draw[darkblue] (4,0)--(4,2.5) node[above]{$a(2)$};
\filldraw[darkblue] (4,2.5) circle (0.05);
\draw[darkblue] (6,0)--(6,2) node[above]{$a(3)$};
\filldraw[darkblue] (6,2) circle (0.05);
}

\subsection{Functions as vectors}
\tikz{
\draw[->, very thick] (0,0)--(7,0) node[right]{$x$};
\draw[->, very thick] (0,0)--(0,3) node[above]{$f(x)$};
\draw[darkblue, thick] (0,1.8) .. controls (1.5,3) and (4,0) .. (6,1.5);
}

Now the reverse. From the function $f$ we can construct a vector
$\ket{f}$, so $f$ is the representation of $\ket{f}$. The analogy:
\begin{align*}
\ket{a}& & \ket{f}&\\
\braket{x_2}{a} &= a_2 & \braket{x_2}{f} &= f(2)\\
\braket{a}{b} &= \sum_k \bar{a_k}b_k & \braket{f}{g} &= ???
\end{align*}
The problem is that the domain of $f$ is continuous, so we can't just
form a sum for $\braket{f}{g}$ as we did for $\braket{a}{b}$ because it
would be an infinite sum that in general does not converge. Instead we
shall do this
\begin{align}\label{eq:innerp}
\braket{f}{g} = \int \bar{f(x)}g(x)\,dx
\end{align}
This preserves the spirit of the dot product, the only change being the
weight $dx$ which converts infinite quantities to finite.

Some notation notes: $\braket{x_2}{f} = f(2)$, $\braket{x_\pi}{f} =
f(\pi)$, etc., but to avoid the awkward notation of $\braket{x_x}{f} =
f(x)$, we shall just write $\braket{x}{f} = f(x)$.

Continuing with \eqref{eq:innerp}
\begin{align*}
\braket{f}{g} &= \int \bar{f(x)}g(x)\,dx\\
&= \int \braket{f}{x}\braket{x}{g}\,dx\\
&= \bra{f} \left(\int \ketbra{x}{x}\,dx\right) \ket{g}
\end{align*}
we see that
\begin{align}\label{eq:inf_id}
1 = \int \ketbra{x}{x}\,dx
\end{align}
analogous to \eqref{eq:finite_id}.

\subsection{Dirac's delta function}
What is $\braket{x}{x_\pi}$ or $\braket{x}{x_1}$? It's going to be a
function, just like $\braket{x}{f} = f(x)$ is a function. So let's just
write $\delta_\pi(x) = \braket{x}{x_\pi}$, $\delta_2(x) =
\braket{x}{x_2}$, etc.

Using \eqref{eq:inf_id}
\begin{align*}
\bar{f(\pi)} = \braket{f}{x_\pi} &= \int
\braket{f}{x}\braket{x}{x_\pi}\,dx\\
&= \int \bar{f(x)} \delta_\pi(x)\,dx
\end{align*}
Since this works for any $f$, we must have
\footnote{Unnecessary rigor is avoided. It is possible to avoid
defining the $\delta$ function completely and continue on without it,
but the analogy with vectors in finite dimensional spaces is lost.
Intuition is more important than rigor, and the $\delta$ function is
a very useful idea.}
\begin{align*}
\braket{x}{x_\pi} = \delta_\pi(x) = \begin{cases}
\frac{1}{dx} & \text{for } \pi-\frac{dx}{2} \leq x \leq
\pi+\frac{dx}{2}\\
0 & \text{everywhere else}
\end{cases}
\end{align*}

How small/What is $dx$? $dx$ shall be a quantity that is smaller than the
precision of your data collecting tools (ruler, weight scale,
voltmeter, experimental apparati) or if you're thinking about it
theoretically, smaller than anything you can imagine.

Notation notes: Instead of writing $\delta_\pi(x)$ or $\delta_2(x)$, we
can just write $\delta_0(x-\pi)$ and $\delta_0(x-2)$. Then we can just
drop the subscript $0$ and just write $\delta(x)$ to mean $\delta_0(x)$
and $\braket{x}{x_\pi} = \delta(x-\pi) = \delta_\pi(x)$, etc.

\tikz{
\draw[->, very thick] (-1,0)--(5,0) node[right]{$x$};
\draw[->, very thick] (0,-1)--(0,4) node[above]{$\delta(x-\pi)$};
\draw[black] (3.14,0) node[below, black]{$\pi$} rectangle (3.24,3.5)
node[midway, right]{$\frac{1}{dx}$} node[above]{$dx$};
}

\subsection{The differential operator}
So we've established the notation $f(x) = \braket{x}{f}$. How are we
going to write $\odx{f} = f'(x)$? $f'(\pi) = (f(\pi+dx) - f(\pi))/dx$,
which is $(\braket{x_{\pi+dx}}{f} - \braket{x_\pi}{f})/dx$. Take a look
at \eqref{eq:finite_deriv}, where we have let $D$ be the giant matrix

\begin{align}\begin{split}\label{eq:finite_deriv}
D \ket{a} &=
\begin{pmatrix}
-1 & 1 & 0 & 0 & 0\\
0 & -1 & 1 & 0 & 0\\
0 & 0 & -1 & 1 & 0\\
0 & 0 & 0 & -1 & 1\\
0 & 0 & 0 & 0 & 1\\
\end{pmatrix}
\begin{pmatrix}
a_1\\
a_2\\
a_3\\
a_4\\
a_5
\end{pmatrix}\\
&=
\begin{pmatrix}
a_2 - a_1\\
a_3 - a_2\\
a_4 - a_3\\
a_5 - a_4\\
a_5
\end{pmatrix}
\end{split}\end{align}
Apart from the last element of $D\ket{a}$, all other elements are of
the form $\bra{x_k}D\ket{a} = \braket{x_{k+1}}{a} - \braket{x_k}{a}$.
Comparing with $f'(\pi) = (\braket{x_{\pi+dx}}{f} -
\braket{x_\pi}{f})/dx$, we can imagine $\odx{}$ to be a matrix
that looks kinda like $D$ in \eqref{eq:finite_deriv}.
\begin{align}
f'(x) = \bra{x}\odx{}\ket{f}
\end{align}

What about derivative of the delta function, like
$\bra{x}\odx{}\ket{x_\pi} = \delta'(x - \pi)$? It's actually more useful
to look at dot products involving the delta function's derivative.
\begin{align*}
\bra{f}\odx{}\ket{x_\pi} &= \int
\braket{f}{x}\bra{x}\odx{}\ket{x_\pi}\,dx\\
&= \int \bar{f(x)} \delta'(x-\pi)\,dx\\
\intertext{This can be integrated by parts.}
&= \left[\bar{f(x)} \delta(x-\pi)\right] - \int \bar{f'(x)}
\delta(x-\pi)\,dx\\
\intertext{The delta function is $0$ at the limits of integration so}
&= -\int \bar{f'(x)}\delta(x-\pi)\,dx\\
&= -\bar{f'(\pi)}\\
&= -\bar{\bra{x_\pi}\odx{}\ket{f}}\\
&= -\bra{f}\bar{\odx{}}\ket{x_\pi}
\end{align*}

So we have established the following $2$ relations
\begin{align}
\bra{f}\odx{}\ket{x_\pi} &= -\bar{f'(\pi)}\\
\intertext{and}
\bar{\odx{}} &= -\odx{}
\end{align}

\section{Change of basis}
\subsection{Rotations}
\tikz{
\draw[->, ultra thick] (0,0)--(2,0) node[right]{$\vec{x_1}$};
\draw[->, ultra thick] (0,0)--(0,2) node[right]{$\vec{x_2}$};
\draw[->, ultra thick, darkgreen] (0,0)--(1.73,1) node[right]{$\vec{y_1}$};
\draw[->, ultra thick, darkgreen] (0,0)--(-1,1.73) node[right]{$\vec{y_2}$};
\draw (0.8,0) arc (0:30:0.8);
\draw (1,0) node[above]{$\theta$};
\draw[->, blue, very thick] (3,1)--(4,2.5) node[right]{$\vec{a}$};
}
Suppose we know the representation of a vector $\ket{a}$ in the
$x$-basis and we want to know the representation in a rotated $y$-basis.
We can use $\eqref{eq:finite_id}$ to compute
\begin{align}
\braket{y_1}{a} &= \sum_k \braket{y_1}{x_k}\braket{x_k}{a}\\
\braket{y_2}{a} &= \sum_k \braket{y_2}{x_k}\braket{x_k}{a}
\end{align}
This is just matrix multiplication.
\begin{align*}
\begin{pmatrix}
\braket{y_1}{a}\\
\braket{y_2}{a}
\end{pmatrix}
=
\begin{pmatrix}
\braket{y_1}{x_1} & \braket{y_1}{x_2}\\
\braket{y_2}{x_1} & \braket{y_2}{x_2}\\
\end{pmatrix}
\begin{pmatrix}
\braket{x_1}{a}\\
\braket{x_2}{a}
\end{pmatrix}
\end{align*}
The matrix $\braket{y_i}{x_j}$ is the \emph{transformation matrix}. The
columns tell you the representation of each $\ket{x_j}$ in the
$y$-basis. For rotations, we see from the picture that the
transformation matrix is
\begin{align*}
\begin{pmatrix}
\cos\theta & \sin\theta\\
-\sin\theta & \cos\theta
\end{pmatrix}
\end{align*}

\subsection{Functions}
Suppose we know $f(x) = \braket{x}{f}$ and we have a new basis
$\ket{y}$. The representation of $\ket{f}$ in the new basis will be a
new function $\tilde{f}(y) = \braket{y}{f}$. We can use
\eqref{eq:inf_id} to compute
\begin{align}
\tilde{f}(y) = \braket{y}{f} &= \int
\braket{y}{x}\braket{x}{f}\,dx\nonumber\\
&= \int \braket{y}{x} f(x)\,dx
\end{align}
We have the transformation matrix $\braket{y}{x}$, a function of two
variables.

\subsection{The Fourier basis}
A particularly important class of basis changings is the Fourier
transforms. These transforms all have the flavor of analyzing periodic
things and frequency and sine waves and stuff and their transformation
matrices all involve roots of unity.\footnote{From the point of view of
groups, they are characters of a cyclic group. The concept of Fourier
transforms can be generalized to more than just cyclic groups.}

\subsubsection{Discrete Fourier transform (DFT)}
Suppose we have a vector $\ket{a}$ in an $n$-dimensional space and we
know its representation in the $x$-basis. We define the $y$-basis with
the transformation matrix
\begin{align}
\braket{y_j}{x_k} = \frac{1}{\sqrt{n}}\exp\left(\frac{-2\pi i}{n}
kj\right)
\end{align}

The representation of $\ket{a}$ in the $y$-basis is given by
\begin{align}
\tilde{a}_j = \braket{y_j}{a} &= \sum_k
\braket{y_j}{x_k}\braket{x_k}{a}\nonumber\\
&= \frac{1}{\sqrt{n}} \sum_k \exp\left(\frac{-2\pi i}{n} kj\right) a_k
\end{align}
which is the \emph{discrete Fourier transform} of $(a_1,\dotsc,
a_n)$.

\subsubsection{Fourier transform}
Suppose we have a function $f(x)$, which is the representation of
$\ket{f}$ in the $x$-basis. Define the $y$-basis\footnote{A basis for a
restricted class of functions} with the transformation matrix
\begin{align}
\braket{y}{x} = \frac{1}{\sqrt{2\pi}} e^{ixy}
\end{align}

The representation of $\ket{f}$ in the $y$-basis is given by
\begin{align}
\tilde{f}(y) = \braket{y}{f} &= \int
\braket{y}{x}\braket{x}{f}\,dx\nonumber\\
&= \frac{1}{\sqrt{2\pi}} \int e^{ixy} f(x)\,dx
\end{align}
which is the \emph{Fourier transform} of the function $f(x)$.

\section{Variational calculus as multivariable calculus}
\subsection{The flavor of variational calculus}


\subsection{Euler-Lagrange equation as a gradient}


\subsection{Application to physics}
\subsubsection{The Lagrangian and Newton's second law}
Let $L(x(t), \dot{x}(t), t) = \frac{1}{2}m\dot{x}^2 - U(x)$. The
Euler-Lagrange equation yields $F = m\ddot{x} = ma$, a.k.a.\ Newton's
second law. The quantity $(\text{kinetic energy} - \text{potential
energy})$ is the \emph{Lagrangian}.

\subsubsection{The Hamiltonian and Hamilton's equations of motion}
Computing the derivatives with chain rule and product rule
\begin{align*}
\odt{L} &= \dot{x}\pdx{L} + \ddot{x}\pd{L}{\dot{x}} + \pdt{L}\\
\odt{}\left(\dot{x}\pd{L}{\dot{x}}\right) &= \ddot{x}\pd{L}{\dot{x}} +
\dot{x}\odt{}\pd{L}{\dot{x}}
\end{align*}

We can form
\begin{align*}
\odt{L} - \odt{}\left(\dot{x}\pd{L}{\dot{x}}\right) =
\dot{x}\left(\pdx{L} - \odt{}\pd{L}{\dot{x}}\right) + \pdt{L}
\end{align*}
which by Euler-Lagrange becomes
\begin{align*}
\odt{}\left(L - \dot{x}\pd{L}{\dot{x}}\right) = \pdt{L}
\end{align*}
The quantity $\dot{x}\pd{L}{\dot{x}} - L$ is the
\emph{Hamiltonian} (denoted $H$), which we see is a constant of motion
as long as $\pdt{L} = 0$, i.e.\ $L$ does not involve time explicitly.

Define $p = \pd{L}{\dot{x}}$, the \emph{canonical momentum}. The
Hamiltonian is $H = \dot{x}p - L$. Looking at its differential
\begin{align*}
dH &= p\,d\dot{x} + \dot{x}\,dp - dL\\
&= p\,d\dot{x} + \dot{x}\,dp - \left(\pdx{L}\,dx + p\,d\dot{x} +
\pdt{L}\,dt\right)\\
&= \dot{x}\,dp - \pdx{L}\,dx - \pdt{L}\,dt\\
&= \dot{x}\,dp - \dot{p}\,dx - \pdt{L}\,dt
\end{align*}
where in the last step we have used Euler-Lagrange ($\dot{p} =
\odt{}\pd{L}{\dot{x}} = \pdx{L}$). We can read off some of $H$'s partial
derivatives
\begin{align}\begin{split}\label{eq:hamilton}
\dot{x} &= \pd{H}{p}\\
\dot{p} &= -\pd{H}{x}
\end{split}\end{align}
which are \emph{Hamilton's equations of motion}.

\subsubsection{The Poisson bracket}
Suppose we have a dynamical variable $b(x,p)$, which could be kinetic
energy, position, momentum, angular momentum (if you extend this to 3
dimensions), whatever. Knowing the initial value of $b$ and $\odt{b}$
for all time would solve physics.
\begin{align*}
\odt{b} &= \pdx{b}\dot{x} + \pd{b}{p}\dot{p}\\
&= \pdx{b}\pd{H}{p} - \pd{b}{p}\pdx{H}
\end{align*}
using \eqref{eq:hamilton}.

Define the \emph{Poisson bracket}
\begin{align}
[m, n] = \pdx{m}\pd{n}{p} - \pdx{n}\pd{m}{p}
\end{align}
Then we have
\begin{align}\label{eq:classical_eom}
\odt{b} = [b, H]
\end{align}
If you know the Hamiltonian of the system and you know the initial value
of any dynamical variable $b$ (that is explicitly independent of time),
you know $b$ for all time. There is a similar equation for quantum
mechanics that is equivalent to the Schro\"dinger equation.

\hfill \emph{Last modified: Bryance Oyang, \today}

\end{document}
